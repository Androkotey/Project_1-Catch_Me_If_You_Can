{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение моделей (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxiakB8-XqHm"
   },
   "source": [
    "В этом блокноте используются файлы, полученные при помощи предыдущих блокнотов (тренировочный и тестовый наборы данных). Наборы векторизуются, к полученным разреженным матрицам добавляются новые признаки, а затем на полученных данных обучается логистическая регресиия и применяется для классификации тестовых сессий."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eB3Sv6UmYnjN"
   },
   "source": [
    "**Содержание**\n",
    "1. Загрузка предподготовленных данных\n",
    "2. Применение CountVectorizer и TfidfVectorizer\n",
    "3. Функции для добавления фичей\n",
    "4. Применение преобразованных наборов данных для классификации\n",
    "5. Применение новых фичей для классификации\n",
    "6. Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cH7eXuG9ZbKb"
   },
   "source": [
    "### Загрузка предподготовленных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Yp38Wzy1XcLd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import (SGDClassifier, LogisticRegression, \n",
    "                                  LogisticRegressionCV)\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "PATH_TO_DATASET = os.path.join('intermediate_data', 'test_train')\n",
    "PATH_TO_DICT = os.path.join('initial_data', 'site_dict')\n",
    "\n",
    "def write_to_submission_file(predicted_labels, out_file,\n",
    "                             target='target', index_label=\"session_id\"):\n",
    "    ''' Записывает файл для посылки в Kaggle '''\n",
    "\n",
    "    predicted_df = pd.DataFrame(predicted_labels,\n",
    "                                index = np.arange(1, predicted_labels.shape[0] + 1),\n",
    "                                columns=[target])\n",
    "    predicted_df.to_csv(out_file, index_label=index_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MwmRRTW8Znsl"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(PATH_TO_DATASET, 'train_s10_w10_m30_final.pkl'), 'rb') as f:\n",
    "    train_df = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DATASET, 'test_s10_w10_m30_final.pkl'), 'rb') as f:\n",
    "    test_df = pickle.load(f)\n",
    "\n",
    "train_df = train_df.sort_values(by=['year', 'month', 'day', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u9iPq4vmZ12j"
   },
   "outputs": [],
   "source": [
    "train_test_df = pd.concat([train_df, test_df])\n",
    "train_test_df_sites = train_test_df[['site%d' % i for i in range(1, 11)]].fillna(0).astype('int')\n",
    "y_train = train_df['target'].astype('int').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwVdY6kdaH_A"
   },
   "source": [
    "### Применение CountVectorizer и TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "4uAPA1AybF7i"
   },
   "outputs": [],
   "source": [
    "sites = ['site%s' % i for i in range(1, 11)]\n",
    "\n",
    "train_df[sites].fillna(0).astype('int').to_csv(os.path.join('intermediate_data', 'train_sessions_text_cnt.txt'), \n",
    "                                               sep=' ', \n",
    "                       index=None, header=None)\n",
    "test_df[sites].fillna(0).astype('int').to_csv(os.path.join('intermediate_data', 'test_sessions_text_cnt.txt'), \n",
    "                                              sep=' ', \n",
    "                       index=None, header=None)\n",
    "\n",
    "with open(os.path.join(PATH_TO_DICT, 'site_dict.pkl'), 'rb') as file:\n",
    "    sites_dict = pickle.load(file)\n",
    "reversed_dict = {v: k for k, v in sites_dict.items()}\n",
    "reversed_dict[0] = '0'\n",
    "\n",
    "train_df[sites].fillna(0).astype('int').applymap(\n",
    "    lambda x: reversed_dict[x]).to_csv(os.path.join('intermediate_data', 'train_sessions_text_tfidf.txt'), sep=' ', \n",
    "    index=None, header=None)\n",
    "test_df[sites].fillna(0).astype('int').applymap(\n",
    "    lambda x: reversed_dict[x]).to_csv(os.path.join('intermediate_data', 'test_sessions_text_tfidf.txt'), sep=' ', \n",
    "    index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBMxhFNbbfhB",
    "outputId": "54a9e470-56e9-4ba7-9070-2f7840cac1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cnt_vectorizer = CountVectorizer(ngram_range=(1, 3), max_features=50000)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5), max_features=100000, tokenizer=lambda s: s.split())\n",
    "with open(os.path.join('intermediate_data', 'train_sessions_text_cnt.txt')) as inp_train_file:\n",
    "    X_train = cnt_vectorizer.fit_transform(inp_train_file)\n",
    "with open(os.path.join('intermediate_data', 'train_sessions_text_tfidf.txt')) as inp_train_file:\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(inp_train_file)\n",
    "with open(os.path.join('intermediate_data', 'test_sessions_text_cnt.txt')) as inp_test_file:\n",
    "    X_test = cnt_vectorizer.transform(inp_test_file)\n",
    "with open(os.path.join('intermediate_data', 'test_sessions_text_tfidf.txt')) as inp_test_file:\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(inp_test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbZOXzX_bkW5",
    "outputId": "0e96dad7-c53a-437d-911a-c3ab6d336305"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '10 1086',\n",
       " '10 11',\n",
       " '10 11 11',\n",
       " '10 11 12',\n",
       " '10 11 14',\n",
       " '10 11 15',\n",
       " '10 11241',\n",
       " '10 1199',\n",
       " '10 12']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0',\n",
       " '0 0',\n",
       " '0 0 0',\n",
       " '0 0 0 0',\n",
       " '0 0 0 0 0',\n",
       " '0.academia-assets.com',\n",
       " '0.docs.google.com',\n",
       " '0.docs.google.com 0',\n",
       " '0.docs.google.com 0 0',\n",
       " '0.docs.google.com 0 0 0']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61e36ttXlzu5"
   },
   "source": [
    "### Функции для добавления фичей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gaGURkHimsmg"
   },
   "outputs": [],
   "source": [
    "def add_time_features(df, X_sparse):\n",
    "    ''' Добавляет индикаторы (4 признака) утра, дня, вечера и ночи '''\n",
    "\n",
    "    hour = df['start_hour']\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int')\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int')\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int')\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int')\n",
    "    X = sp.hstack([X_sparse, morning.values.reshape(-1, 1), \n",
    "                day.values.reshape(-1, 1), evening.values.reshape(-1, 1), \n",
    "                night.values.reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7cgJguExmJBa"
   },
   "outputs": [],
   "source": [
    "def add_session_timespan(df, X_sparse):\n",
    "    ''' Добавляет продолжительность сессии в секундах (1 признак) '''\n",
    "\n",
    "    session_timespan = df['session_timespan']\n",
    "    X = sp.hstack([X_sparse, session_timespan.values.reshape(-1, 1)])\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g3peMxgamKYC"
   },
   "outputs": [],
   "source": [
    "def feature_short_session(df, X_sparse, time_min=0, time_max=3):\n",
    "    ''' Добавляет индикаторы короткой сессии (от 0 до 2 секунд) \n",
    "    (1 признак)'''\n",
    "\n",
    "    def short_session(x): \n",
    "        if ((x >= time_min) and (x < time_max)):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    times = df['session_timespan']\n",
    "    X = sp.hstack([X_sparse, times.apply(short_session).values.reshape(-1, 1)])\n",
    "    return X\n",
    "    \n",
    "\n",
    "def feature_middle_session(df, X_sparse, time_min=3, time_max=9):\n",
    "    ''' Добавляет индикаторы средней сессии (от 3 до 6 секунд) \n",
    "    (1 признак)'''\n",
    "\n",
    "    def middle_session(x): \n",
    "        if ((x >= time_min) and (x < time_max)):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    times = df['session_timespan']\n",
    "    X = sp.hstack([X_sparse, times.apply(middle_session).values.reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "9rerr5M70bLb"
   },
   "outputs": [],
   "source": [
    "def add_top_sites(df, X_sparse, list_of_sites={3000, 2080, 27307}):\n",
    "    ''' Добавляет индикаторы (1 признак) нескольких сайтов, которые Alice \n",
    "    использует замето чаще, чем остальные люди '''\n",
    "    \n",
    "    sites = df[['site%s' % i for i in range(1, 11)]].values\n",
    "    result = [0]*sites.shape[0]\n",
    "\n",
    "    for i, session in enumerate(sites):\n",
    "        for site in session:\n",
    "            if site in list_of_sites:\n",
    "                result[i] += 1\n",
    "    \n",
    "    X = sp.hstack([X_sparse, np.array(result).reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Cew_rZAF0csV"
   },
   "outputs": [],
   "source": [
    "def add_reverse_top_sites(df, X_sparse, list_of_sites={55, 56, 570, \n",
    "                                                       778, 780, 782, \n",
    "                                                       786, 812}):\n",
    "    ''' Добавляет индикаторы (1 признак) нескольких сайтов, которые Alice \n",
    "    использует замето реже, чем остальные люди '''\n",
    "\n",
    "    sites = df[['site%s' % i for i in range(1, 11)]].values\n",
    "    result = [0]*sites.shape[0]\n",
    "\n",
    "    for i, session in enumerate(sites):\n",
    "        for site in session:\n",
    "            if site in list_of_sites:\n",
    "                result[i] += 1\n",
    "    \n",
    "    X = sp.hstack([X_sparse, np.array(result).reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "umTZWHHNOiKZ"
   },
   "outputs": [],
   "source": [
    "''' Оказалось, что признак плохой и приводит к переобучению '''\n",
    "\n",
    "def add_start_hour(df, X_sparse):\n",
    "    ''' Добавляет час начала сессии (1 признак) '''\n",
    "\n",
    "    start_hour = df['start_hour']\n",
    "    X = sp.hstack([X_sparse, start_hour.values.reshape(-1, 1)])\n",
    "    return X\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3sTwQvOOOuXD"
   },
   "outputs": [],
   "source": [
    "def add_day_of_week(df, X_sparse):\n",
    "    ''' Добавляет день начала сессии (1 признак) '''\n",
    "\n",
    "    day_of_week = df['day_of_week']\n",
    "    X = sp.hstack([X_sparse, day_of_week.values.reshape(-1, 1)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCenXXQNn6w9"
   },
   "source": [
    "Далее закомментированы определения функций, добавляющих плохие признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Xo0Roc0UmPPz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef add_num_of_unique(df, X_sparse):\\n    # Добавляет количество уникальных сайтов в сессии (1 признак)\\n\\n    num_of_unique = df['#unique_sites']\\n    X = sp.hstack([X_sparse, num_of_unique.values.reshape(-1, 1)])\\n    return X\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def add_num_of_unique(df, X_sparse):\n",
    "    # Добавляет количество уникальных сайтов в сессии (1 признак)\n",
    "\n",
    "    num_of_unique = df['#unique_sites']\n",
    "    X = sp.hstack([X_sparse, num_of_unique.values.reshape(-1, 1)])\n",
    "    return X\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xUWGvUbwmvhc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.preprocessing import StandardScaler\\n\\ndef add_session_timespan_scaled(df, X_sparse):\\n    session_timespan = df['session_timespan_scaled']\\n    X = sp.hstack([X_sparse, session_timespan.values.reshape(-1, 1)])\\n    return X\\n    \\nscaler_sess_timespan = StandardScaler(with_std=False, with_mean=False)\\ntrain_df['session_timespan_scaled'] = scaler_sess_timespan.fit_transform(train_df['session_timespan'].values.reshape(-1, 1))\\ntest_df['session_timespan_scaled'] = scaler_sess_timespan.transform(test_df['session_timespan'].values.reshape(-1, 1))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def add_session_timespan_scaled(df, X_sparse):\n",
    "    session_timespan = df['session_timespan_scaled']\n",
    "    X = sp.hstack([X_sparse, session_timespan.values.reshape(-1, 1)])\n",
    "    return X\n",
    "    \n",
    "scaler_sess_timespan = StandardScaler(with_std=False, with_mean=False)\n",
    "train_df['session_timespan_scaled'] = scaler_sess_timespan.fit_transform(train_df['session_timespan'].values.reshape(-1, 1))\n",
    "test_df['session_timespan_scaled'] = scaler_sess_timespan.transform(test_df['session_timespan'].values.reshape(-1, 1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yewtHTuIm2f4"
   },
   "source": [
    "### Применение преобразованных наборов данных для классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTXoQNpBpmlA"
   },
   "source": [
    "В данном разделе были обучены LogisticRegressionCV на двух тренировочных наборах данных:\n",
    "- К тренировочным и тестовым наборам, преобразованным при помощи CountVectorizer и TfidfVectorizer добавлены индикаторы утра, дня, вечера и ночи.\n",
    "- Определены показатели метрики roc-auc.\n",
    "- Обученные LogisticRegressionCV с найденным коэффициентом регуляризации прменены для классификации сессий в соответствующих тестовых наборах данных.\n",
    "- Отправлены посылки в соревновании на Kaggle.\n",
    "- Сделаны выводы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ZrXMr7MdnCxI"
   },
   "outputs": [],
   "source": [
    "time_split = TimeSeriesSplit(n_splits=10) # для валидации\n",
    "y_train = train_df['target'].astype('int').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eLaxTNKSnfyO"
   },
   "outputs": [],
   "source": [
    "X_train_cnt_1f = add_time_features(train_df.fillna(0), X_train)\n",
    "X_test_cnt_1f = add_time_features(test_df.fillna(0), X_test)\n",
    "\n",
    "X_train_tfidf_1f = add_time_features(train_df.fillna(0), X_train_tfidf)\n",
    "X_test_tfidf_1f = add_time_features(test_df.fillna(0), X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6QyvPW3pDbl",
    "outputId": "26d1f4c3-4aef-4957-8c1e-e4634bddd871"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   38.2s remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  1.5min remaining:   39.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Wall time: 2min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([0.1       , 0.64444444, 1.18888889, 1.73333333, 2.27777778,\n",
       "       2.82222222, 3.36666667, 3.91111111, 4.45555556, 5.        ]),\n",
       "                     cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "                     max_iter=2000, n_jobs=-1, random_state=17,\n",
       "                     scoring='roc_auc', solver='liblinear', verbose=3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_c_values = np.linspace(0.1, 5, 10)\n",
    "\n",
    "logit_grid_searcher_cnt = LogisticRegressionCV(Cs=logit_c_values, \n",
    "                                            solver='liblinear', \n",
    "                                            random_state=17,\n",
    "                                            cv=time_split,\n",
    "                                            n_jobs=-1, scoring='roc_auc',\n",
    "                                            verbose=3,\n",
    "                                            max_iter=2000)\n",
    "logit_grid_searcher_cnt.fit(X_train_cnt_1f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LJq0gLFphwp",
    "outputId": "dd3b57ea-a068-4487-ce56-3b8233865164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166806003300598 0.6444444444444445\n"
     ]
    }
   ],
   "source": [
    "logit_mean_cv_scores_cnt = logit_grid_searcher_cnt.scores_[1].mean(axis=0)\n",
    "print(logit_mean_cv_scores_cnt.max(), \n",
    "      logit_grid_searcher_cnt.Cs_[logit_mean_cv_scores_cnt.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "W4dH8MpysMsF"
   },
   "outputs": [],
   "source": [
    "logit_test_pred_cnt= logit_grid_searcher_cnt.predict_proba(X_test_cnt_1f)[:, 1]\n",
    "write_to_submission_file(logit_test_pred_cnt, 'submit_cnt.csv') # 0.93980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8BHk8rNpSG2",
    "outputId": "0f24ca63-f164-430a-d78d-6155921b41d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:   16.6s remaining:   38.8s\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:   29.2s remaining:   12.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:   36.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]Wall time: 39.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegressionCV(Cs=array([0.1       , 0.64444444, 1.18888889, 1.73333333, 2.27777778,\n",
       "       2.82222222, 3.36666667, 3.91111111, 4.45555556, 5.        ]),\n",
       "                     cv=TimeSeriesSplit(max_train_size=None, n_splits=10),\n",
       "                     max_iter=2000, n_jobs=-1, random_state=17,\n",
       "                     scoring='roc_auc', solver='liblinear', verbose=3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "logit_c_values = np.linspace(0.1, 5, 10)\n",
    "\n",
    "logit_grid_searcher_tfidf = LogisticRegressionCV(Cs=logit_c_values, \n",
    "                                            solver='liblinear', \n",
    "                                            random_state=17,\n",
    "                                            cv=time_split,\n",
    "                                            n_jobs=-1, scoring='roc_auc',\n",
    "                                            verbose=3,\n",
    "                                            max_iter=2000)\n",
    "logit_grid_searcher_tfidf.fit(X_train_tfidf_1f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BbLjkRv0piSP",
    "outputId": "37aa76de-cf9c-45fb-8b83-699a2c8f2b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9245491008379615 4.455555555555556\n"
     ]
    }
   ],
   "source": [
    "logit_mean_cv_scores_tfidf = logit_grid_searcher_tfidf.scores_[1].mean(axis=0)\n",
    "print(logit_mean_cv_scores_tfidf.max(), \n",
    "      logit_grid_searcher_tfidf.Cs_[logit_mean_cv_scores_tfidf.argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "dhFsOGBxsNVV"
   },
   "outputs": [],
   "source": [
    "logit_test_pred_tfidf = logit_grid_searcher_tfidf.predict_proba(X_test_tfidf_1f)[:, \n",
    "1]\n",
    "write_to_submission_file(logit_test_pred_tfidf, 'submit_tfidf.csv') # 0.94368"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5RTiL0v6thHe"
   },
   "source": [
    "Поскольку более высокому roc-auc в leaderboard у TfidfVectorizer соответствует более высокое значение данной метрики и на валидации, то можно сделать вывод об удачном выборе метода валидации. В дальнейшем будут использоваться наборы данных, преобразованные при помощи TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "gQYrM73uuJTM"
   },
   "outputs": [],
   "source": [
    "X_train_1f = X_train_tfidf_1f\n",
    "X_test_1f = X_test_tfidf_1f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tiTRMcj9utXV"
   },
   "source": [
    "### Применение новых фичей для классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZOuDjZgfu1Fp"
   },
   "outputs": [],
   "source": [
    "X_train_2f = add_session_timespan(train_df, X_train_1f)\n",
    "X_test_2f = add_session_timespan(test_df, X_test_1f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qtkq6xGVvXTO"
   },
   "outputs": [],
   "source": [
    "X_train_3f = feature_short_session(train_df, X_train_2f)\n",
    "X_test_3f = feature_short_session(test_df, X_test_2f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tpyV52aSvrkf"
   },
   "outputs": [],
   "source": [
    "X_train_4f = add_top_sites(train_df, X_train_3f)\n",
    "X_test_4f = add_top_sites(test_df, X_test_3f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3dqEqkJLwI26"
   },
   "outputs": [],
   "source": [
    "X_train_5f = add_reverse_top_sites(train_df, X_train_4f)\n",
    "X_test_5f = add_reverse_top_sites(test_df, X_test_4f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_6f = feature_middle_session(train_df, X_train_5f)\n",
    "X_test_6f = feature_middle_session(test_df, X_test_5f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l-VQ_1aZyh9w",
    "outputId": "07657228-99bd-4a0f-ca52-d1695b427e7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  10 | elapsed:  4.1min remaining:  9.6min\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:  6.8min remaining:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:  9.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.9321137656775317 3.6530612244897958\n"
     ]
    }
   ],
   "source": [
    "# Пример кросс-валидации\n",
    "logit_c_values = np.linspace(3, 5, 50)\n",
    "\n",
    "logit_final = LogisticRegressionCV(Cs=logit_c_values, \n",
    "                                   solver='liblinear', \n",
    "                                   random_state=17,\n",
    "                                   cv=time_split,\n",
    "                                   n_jobs=-1, scoring='roc_auc',\n",
    "                                   verbose=3,\n",
    "                                   penalty='l2')\n",
    "logit_final.fit(X_train_6f, y_train)\n",
    "\n",
    "logit_mean_cv_scores = logit_final.scores_[1].mean(axis=0)\n",
    "print(logit_mean_cv_scores.max(), logit_final.Cs_[logit_mean_cv_scores.argmax()])\n",
    "\n",
    "# 0.9321137656775317 3.6530612244897958 # 0.95044"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bBSOY4Yb07FR",
    "outputId": "1322062e-87b5-415f-fe7d-6752cc92c215"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.6530612244897958, n_jobs=-1, random_state=17,\n",
       "                   solver='liblinear')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_final = LogisticRegression(C=3.6530612244897958, \n",
    "                                 solver='liblinear', \n",
    "                                 random_state=17, \n",
    "                                 n_jobs=-1, \n",
    "                                 penalty='l2')\n",
    "logit_final.fit(X_train_6f, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5TnNXApLK2j",
    "outputId": "05909c8b-e3c5-4ac2-e0db-14a29fdc5bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final cv score: 0.9310553238208839\n"
     ]
    }
   ],
   "source": [
    "logit_final_cv_score = cross_val_score(logit_final, \n",
    "                                       X_train_6f, \n",
    "                                       y_train, \n",
    "                                       scoring='roc_auc', \n",
    "                                       cv=time_split).mean()\n",
    "print('final cv score:', logit_final_cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "_60hoNqFyx26"
   },
   "outputs": [],
   "source": [
    "logit_test_pred_final = logit_final.predict_proba(X_test_6f)[:, 1]\n",
    "write_to_submission_file(logit_test_pred_final, 'subm_logit_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score на публичной части тестовых данных: **0.95044**. Результат далёк от лучших позиций в рейтинге, однако целью проделанной работы являлось знакомство с площадкой Kaggle, получение навыков обработки и визуализации данных, а также изучение основ машинного обучения.\n",
    "\n",
    "В качестве шагов для улучшения качества построенной модели в дальнейшем будут проделаны следующие шаги:\n",
    "* применение других методов машинного обучения (бустинг над решающими деревьями, метод ближайших соседей);\n",
    "* улучшения в схеме кросс-валидации (замечено, что не всегда рост roc-auc на кросс-валидации соответствует росту roc-auc на публичной части тестовых данных);\n",
    "* проверка на переобученность модели при использовании отдельных признаков."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
