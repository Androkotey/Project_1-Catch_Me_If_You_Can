{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная глава (4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном блокноте собраны функции, позволяющие обработать наборы данных о посещениях сайтов пользователями. Сырые данные для этой задачи представляют собой cvs-файлы с данными о веб-сёрфинге отдельного пользователя в следующем виде: \n",
    "```\n",
    "timestamp,site\n",
    "2013-11-15 11:40:34,google.com\n",
    "...\n",
    "```\n",
    "Требуется объединить коллекцию таких файлов в одну большую таблицу. При этом в полученной таблице отдельные строки - это сессии - последовательности из нескольких сайтов. Решение должно поддерживать создание таблицы с разной длиной сессии, а также с различной шириной окна.\n",
    "\n",
    "**Пример**: для длины сессии 10 и ширины окна 7 файл из 30 записей породит не 3 сессии (1-10, 11-20, 21-30), а 5 (1-10, 8-17, 15-24, 22-30, 29-30). При этом в предпоследней сессии будет один ноль, а в последней – 8 нолей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">Важное замечание!</span> Задача, поставленная в этом разделе не связана напрямую с решением предложенной организаторами соревнования на Kaggel. Задача решалась исключительно в факультативных целях и для того, чтобы осознать необходимость оптимизации кода."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from glob import glob\n",
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import re\n",
    "import scipy.sparse as sp\n",
    "import time\n",
    "\n",
    "PATH_TO_DATA = os.path.join('initial_data', 'users')\n",
    "PATH_TO_SITE_FREQ = os.path.join('initial_data', 'site_freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на файл с данными на примере пользователя user0128 из коллекции с 10 пользователями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-11-15 13:46:03</td>\n",
       "      <td>fpdownload2.macromedia.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-11-15 13:46:13</td>\n",
       "      <td>mail.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-11-15 13:46:13</td>\n",
       "      <td>www.gmail.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-11-15 13:46:25</td>\n",
       "      <td>accounts.google.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-11-15 13:46:28</td>\n",
       "      <td>accounts.youtube.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                        site\n",
       "0  2013-11-15 13:46:03  fpdownload2.macromedia.com\n",
       "1  2013-11-15 13:46:13             mail.google.com\n",
       "2  2013-11-15 13:46:13               www.gmail.com\n",
       "3  2013-11-15 13:46:25         accounts.google.com\n",
       "4  2013-11-15 13:46:28        accounts.youtube.com"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user31_data = pd.read_csv(os.path.join(PATH_TO_DATA, '10users/user0128.csv'))\n",
    "user31_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основная функция в этом разделе - это функция\n",
    "```\n",
    "prepare_sparse_train_set_window.\n",
    "```\n",
    "Она принимает на вход путь к коллекции csv-файлов с данными по каждому пользователю, путь к заранее подготовленному словарю сайтов, а также параметры session_length и window_size, отвечающие за длину сессии и размер окна соответственно.\n",
    "\n",
    "Для удобства, отдельно определена функция\n",
    "```\n",
    "make_sparse_data,\n",
    "```\n",
    "которая помогает на основе таблицы с сессиями создаёт матрицу частот в разреженном формате."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_data(data):\n",
    "    ''' Принимает на вход DataFrame с сессиями и \n",
    "    возвращает разреженную матрицу частот'''\n",
    "    \n",
    "    indptr = [0]\n",
    "    indices = []\n",
    "    sparse_data = []\n",
    "    for row in tqdm(data):\n",
    "        val, cnt = np.unique(row[row != 0], return_counts=True)\n",
    "        indptr.append(indptr[-1] + len(val))\n",
    "        for v, c in zip(val, cnt):\n",
    "            indices.append(v - 1)\n",
    "            sparse_data.append(c)\n",
    "            \n",
    "    return np.uint64(sparse_data),  np.uint64(indices), np.uint64(indptr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_set_window(path_to_csv_files, site_freq_path, \n",
    "                            session_length=10, window_size=10):\n",
    "    ''' Подготавливает набор данных. Сырые данных хранятся в path_to_csv_files. \n",
    "    Возвращает матрицу частот в разреженном формате, вектор с user_id и DataFrame\n",
    "    с соответствующим набором сессий'''\n",
    "    \n",
    "    with open(site_freq_path, 'rb') as file:\n",
    "        site_freq_dict = pickle.load(file)\n",
    " \n",
    "    data = []\n",
    "    \n",
    "    list_of_files = glob(os.path.join(path_to_csv_files, '*'))         \n",
    "    for path_to_user in list_of_files:\n",
    "        user_id = int(re.sub(r'\\b0+', '', path_to_user[-8:-4], 1)) # user0001 --> 1\n",
    "        sites_array = pd.read_csv(path_to_user)['site'].map(lambda x: site_freq_dict[x][0]).values.tolist()\n",
    "        n = len(sites_array)\n",
    "        ind = 0\n",
    "        while True:\n",
    "            if ind + session_length > n-1:\n",
    "                data.append(sites_array[ind:n] + [0 for _ in range(ind + session_length - n)] + [user_id])\n",
    "            else:\n",
    "                data.append(sites_array[ind:ind+session_length] + [user_id])\n",
    "            ind += window_size\n",
    "            if ind >= n:\n",
    "                break\n",
    "    \n",
    "    feature_names=[f'site{i}' for i in range(1, session_length + 1)] + ['target']\n",
    "    data_df = pd.DataFrame(data, columns=feature_names)\n",
    "    target = np.array(data_df['target'].values, dtype='int16')\n",
    "    \n",
    "    '''Создание и заполнение разреженной матрицы частот'''\n",
    "    X, y = data_df.iloc[:, :-1].values, data_df.iloc[:, -1].values\n",
    "    sparse_data = sp.csr_matrix(make_sparse_data(X), dtype='int8')\n",
    "     \n",
    "    return sparse_data, target, data_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195712/195712 [00:06<00:00, 30078.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_sparse_10users_s10_w7, y_10users_s10_w7, df = prepare_data_set_window(os.path.join(PATH_TO_DATA,'150users'), \n",
    "                                                    os.path.join(PATH_TO_SITE_FREQ,'site_freq_150users.pkl'),\n",
    "                                                    session_length=10, window_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обработаем наборы данных для 10 и 150 пользователей и сохраним разреженные матрицы в формате \n",
    "```\n",
    "X_sparse_{кол-во пользователей}users_s{длина сессии}_w{ширина окна}.pkl \n",
    "```\n",
    "Также сохраним id пользователей в формате\n",
    "```\n",
    "y_{кол-во пользователей}users_s{длина сессии}_w{ширина окна}.pkl\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14061/14061 [00:00<00:00, 29978.99it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 14061/14061 [00:00<00:00, 30358.40it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20087/20087 [00:00<00:00, 30053.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20087/20087 [00:00<00:00, 30959.09it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20087/20087 [00:00<00:00, 31821.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28118/28118 [00:00<00:00, 29757.74it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28118/28118 [00:00<00:00, 30645.10it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28118/28118 [00:00<00:00, 31796.35it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28118/28118 [00:00<00:00, 32044.24it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 137019/137019 [00:04<00:00, 28681.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 137019/137019 [00:04<00:00, 29834.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195712/195712 [00:06<00:00, 29247.96it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195712/195712 [00:06<00:00, 30443.86it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 195712/195712 [00:06<00:00, 30196.01it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273957/273957 [00:09<00:00, 29314.67it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273957/273957 [00:09<00:00, 30205.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273957/273957 [00:08<00:00, 31819.93it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 273957/273957 [00:08<00:00, 32502.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for num_users in [10, 150]:\n",
    "    for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]):\n",
    "        if window_size <= session_length:\n",
    "            X_sparse, y, _ = prepare_data_set_window(os.path.join(PATH_TO_DATA,'{}users'.format(num_users)),\n",
    "                                                            os.path.join(PATH_TO_SITE_FREQ,'site_freq_{}users.pkl'.format(num_users)),\n",
    "                                                            session_length=session_length, window_size=window_size)\n",
    "            with open(os.path.join(PATH_TO_DATA, \n",
    "                                   'X_sparse_{}users_s{}_w{}.pkl'.format(num_users, \n",
    "                                                                         session_length, \n",
    "                                                                         window_size)), 'wb') as X_pkl:\n",
    "                pickle.dump(X_sparse, X_pkl)\n",
    "            with open(os.path.join(PATH_TO_DATA,\n",
    "                                   'y_{}users_s{}_w{}.pkl'.format(num_users, \n",
    "                                                                  session_length, \n",
    "                                                                  window_size)), 'wb') as y_pkl:\n",
    "                pickle.dump(y, y_pkl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда подготовлены таблицы с сессиями в различных форматах (с различными session_length и window_size), можно попробовать натренировать модели (например, логистическую регрессию) на различных наборах данных. Тренировать модели на данных 150 пользователей долго, поэтому для примера, воспользуемся датасетами для 10 пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def logit_cv(logit_c_values, path_to_X_pickle, path_to_y_pickle):\n",
    "    ''' Тренирует LogisticRegressionCV с параметром из logit_c_values \n",
    "    на данных из path_to_X_pickle и path_to_y_pickle'''\n",
    "    \n",
    "    with open(path_to_X_pickle, 'rb') as X_sparse_users_pkl:\n",
    "        X_sparse_users = pickle.load(X_sparse_users_pkl)\n",
    "    with open(path_to_y_pickle, 'rb') as y_pkl:\n",
    "        y = pickle.load(y_pkl)\n",
    "\n",
    "    logit_c_values = np.linspace(0.5, 4, 5)\n",
    "\n",
    "    logit_grid_searcher = LogisticRegressionCV(Cs=logit_c_values, \n",
    "                                               multi_class='multinomial', \n",
    "                                               random_state=17,\n",
    "                                               cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=17),\n",
    "                                               n_jobs=-1)\n",
    "    logit_grid_searcher.fit(X_sparse_users, y)\n",
    "\n",
    "    return logit_grid_searcher\n",
    "\n",
    "logit_c_values = np.linspace(0.5, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session_length = 15, window_size = 10\n",
      "max cv score for user0031:  0.8381338453879525\n",
      "best C:  1.375\n",
      "session_length = 10, window_size = 10\n",
      "max cv score for user0031:  0.7763316976032999\n",
      "best C:  1.375\n",
      "session_length = 15, window_size = 7\n",
      "max cv score for user0031:  0.8637924201474676\n",
      "best C:  3.125\n",
      "session_length = 10, window_size = 7\n",
      "max cv score for user0031:  0.8123162268245263\n",
      "best C:  1.375\n",
      "session_length = 7, window_size = 7\n",
      "max cv score for user0031:  0.7587494040411881\n",
      "best C:  3.125\n",
      "session_length = 15, window_size = 5\n",
      "max cv score for user0031:  0.8841665915712135\n",
      "best C:  3.125\n",
      "session_length = 10, window_size = 5\n",
      "max cv score for user0031:  0.8334162722580608\n",
      "best C:  4.0\n",
      "session_length = 7, window_size = 5\n",
      "max cv score for user0031:  0.7878939587540709\n",
      "best C:  3.125\n",
      "session_length = 5, window_size = 5\n",
      "max cv score for user0031:  0.7368590432312572\n",
      "best C:  3.125\n"
     ]
    }
   ],
   "source": [
    "all_logit_searchers = []\n",
    "\n",
    "for window_size, session_length in itertools.product([10, 7, 5], [15, 10, 7, 5]):\n",
    "    if window_size <= session_length:\n",
    "        path_to_X_pickle = os.path.join(PATH_TO_DATA,\n",
    "                                     f'X_sparse_10users_s{session_length}_w{window_size}.pkl')\n",
    "        path_to_y_pickle = os.path.join(PATH_TO_DATA, \n",
    "                                     f'y_10users_s{session_length}_w{window_size}.pkl')\n",
    "        \n",
    "        logit_grid_searcher = logit_cv(logit_c_values, path_to_X_pickle, path_to_y_pickle)\n",
    "        \n",
    "        all_logit_searchers.append(logit_grid_searcher)\n",
    "        \n",
    "        print(f'session_length = {session_length}, window_size = {window_size}')\n",
    "        logit_mean_cv_scores = logit_grid_searcher.scores_[31].mean(axis=0)\n",
    "        print('max cv score for user0031: ', logit_mean_cv_scores.max())\n",
    "        print('best C: ', logit_grid_searcher.Cs_[logit_mean_cv_scores.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наилучшее качество на кросс-валидации оказалось у датасета session_length = 15, window_size = 5. В дальнейшем можно проверить то же самое на более крупных датасетах, например на датасете 3000users. Но это выходит за рамки дополнительной главы, которая на этом заканчивается."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
